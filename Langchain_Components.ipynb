{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install langchain\n",
        "!pip install google-generativeai\n",
        "!pip install langchain-community\n",
        "!pip install langchain_google_genai\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEuMY43bdT0v",
        "outputId": "83718bc0-4a50-46bb-f041-5726d3b872bc"
      },
      "id": "JEuMY43bdT0v",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain)\n",
            "  Downloading langchain_core-0.2.33-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (4.12.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m734.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.33-py3-none-any.whl (391 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.5/391.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.99-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tenacity, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-core-0.2.33 langchain-text-splitters-0.2.2 langsmith-0.1.99 orjson-3.10.7 tenacity-8.5.0\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.2)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.20.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.12 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-1.0.9-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.7.2)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.2.33)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.24.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.32->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.32->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.32->langchain_google_genai) (0.1.99)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.32->langchain_google_genai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.32->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.63.2)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.32->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.32->langchain_google_genai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.20.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (2024.7.4)\n",
            "Downloading langchain_google_genai-1.0.9-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: langchain_google_genai\n",
            "Successfully installed langchain_google_genai-1.0.9\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d788b0",
      "metadata": {
        "id": "11d788b0"
      },
      "source": [
        "### **What is LangChain?**\n",
        "> LangChain is a framework for developing applications powered by language models.\n",
        "\n",
        "**~~TL~~DR**: LangChain makes the complicated parts of working & building with AI models easier. It helps do this in two ways:\n",
        "\n",
        "1. **Integration** - Bring external data, such as your files, other applications, and api data, to your LLMs\n",
        "2. **Agency** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next\n",
        "\n",
        "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
        "\n",
        "*Note: This tutorial will not cover all aspects of LangChain. It's contents have been curated to get you to building & impact as quick as possible. For more, please check out [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
        "\n",
        "You'll need an GoogleGenAI api key to follow this tutorial. which can be obtained from [GoogleGenAI Studio](https://aistudio.google.com/app/apikey)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "P3MrNSAbl2ju"
      },
      "id": "P3MrNSAbl2ju",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e9815081",
      "metadata": {
        "hide_input": false,
        "id": "e9815081"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = 'YourAPIKey'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05bb564d",
      "metadata": {
        "id": "05bb564d"
      },
      "source": [
        "# LangChain Components\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f39eb39",
      "metadata": {
        "id": "2f39eb39"
      },
      "source": [
        "### **Chat Messages**\n",
        "Like text, but specified with a message type (System, Human, AI)\n",
        "\n",
        "* **System** - Helpful background context that tell the AI what to do\n",
        "* **Human** - Messages that are intented to represent the user\n",
        "* **AI** - Messages that show what the AI responded with\n",
        "\n",
        "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "99b0935b",
      "metadata": {
        "id": "99b0935b"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "chat = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d2f7af",
      "metadata": {
        "id": "a2d2f7af"
      },
      "source": [
        "Now let's create a few messages that simulate a chat experience with a bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "878d6a36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "878d6a36",
        "outputId": "6e7c89d0-2f1b-47cc-d369-cf909348c725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='A delicious Caprese salad with fresh mozzarella and basil would be perfect! \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-68350010-838d-430a-9791-df4b382a62f1-0', usage_metadata={'input_tokens': 29, 'output_tokens': 14, 'total_tokens': 43})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
        "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a425aaa",
      "metadata": {
        "id": "0a425aaa"
      },
      "source": [
        "You can also pass more chat history w/ responses from the AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8fd3fe88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fd3fe88",
        "outputId": "03edbc49-639f-463c-cc65-823df91f7a16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'SAFETY', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'MEDIUM', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-2012ef84-8ad2-4b6b-8973-138c1da8e7f3-0', usage_metadata={'input_tokens': 49, 'output_tokens': 0, 'total_tokens': 49})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
        "        HumanMessage(content=\"I like food where should I go?\"),\n",
        "        AIMessage(content=\"You should go to Nice, Hyderabad, India\"),\n",
        "        HumanMessage(content=\"What should I eat when I'm there?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5ee37a",
      "metadata": {
        "id": "ff5ee37a"
      },
      "source": [
        "You can also exclude the system message if you want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "238a49f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "238a49f6",
        "outputId": "7378acf1-b5c3-48f5-ff02-97fbf441a925"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The day that comes after Thursday is **Friday**. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "chat(\n",
        "    [\n",
        "        HumanMessage(content=\"What day comes after Thursday?\")\n",
        "    ]\n",
        ").content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bf9634",
      "metadata": {
        "id": "66bf9634"
      },
      "source": [
        "### **Documents**\n",
        "An object that holds a piece of text and metadata (more information about that text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3bbf58b2",
      "metadata": {
        "id": "3bbf58b2"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6ad9bef6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad9bef6",
        "outputId": "3497e932-d5fd-4170-e9e6-b85f07c36aee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
        "         metadata={\n",
        "             'my_document_id' : 234234,\n",
        "             'my_document_source' : \"The LangChain Papers\",\n",
        "             'my_document_create_time' : 1680013019\n",
        "         })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bd19754",
      "metadata": {
        "id": "3bd19754"
      },
      "source": [
        "But you don't have to include metadata if you don't want to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0798d3ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0798d3ca",
        "outputId": "7dab4743-08fe-442c-9030-200a819ef0be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e462b5d",
      "metadata": {
        "id": "2e462b5d"
      },
      "source": [
        "## Models - The interface to the AI brains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b27fe982",
      "metadata": {
        "id": "b27fe982"
      },
      "source": [
        "###  **Language Model**\n",
        "A model that does text in ➡️ text out!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Language model parameters\n",
        "- **model**: the name of the model\n",
        "- **temperature**: the sampling temperature\n",
        "- **timeout**: request timeout\n",
        "- **max_tokens**: max tokens to generate\n",
        "- **stop**: default stop sequences\n",
        "- **max_retries**: max number of times to retry requests\n",
        "- **api_key**: API key for the model provider\n",
        "- **base_url**: endpoint to send requests to\n",
        "\n",
        "Along with these there can be some model and search specific parameters like **top_k**, **top_p** etc."
      ],
      "metadata": {
        "id": "xZtt83NPr95C"
      },
      "id": "xZtt83NPr95C"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "74b1a72a",
      "metadata": {
        "id": "74b1a72a"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "llm = GoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6399c295",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6399c295",
        "outputId": "51e58538-5333-45f7-852e-98a83d359a67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Saturday comes after Friday. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "llm(\"What day comes after Friday?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef89bfa",
      "metadata": {
        "id": "3ef89bfa"
      },
      "source": [
        "### **Chat Model**\n",
        "A model that takes a series of messages and returns a message output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf091777",
      "metadata": {
        "id": "bf091777"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "chat = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4260711",
      "metadata": {
        "id": "f4260711",
        "outputId": "e7887b18-2008-4203-cc7c-c175fb04d584"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the math book go to New York? Because it had too many problems and needed a change of scenery!')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
        "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c028f9",
      "metadata": {
        "id": "05c028f9"
      },
      "source": [
        "### Function Calling Models\n",
        "\n",
        "[Function calling models](https://openai.com/blog/function-calling-and-other-api-updates) are similar to Chat Models but with a little extra flavor. They are fine tuned to give structured data outputs.\n",
        "\n",
        "This comes in handy when you're making an API call to an external service or doing extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1020ff45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1020ff45",
        "outputId": "12fff61d-bf6c-4e99-b76d-b3f6cbd77f94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Boston, MA\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-34aed25e-1b3e-47e7-aefe-2b80a5790712-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Boston, MA'}, 'id': '2d014497-9959-409a-be28-a4887483d136', 'type': 'tool_call'}], usage_metadata={'input_tokens': 95, 'output_tokens': 19, 'total_tokens': 114})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "output = chat(messages=\n",
        "     [\n",
        "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
        "         HumanMessage(content=\"What’s the weather like in Boston right now?\")\n",
        "     ],\n",
        "     functions=[{\n",
        "         \"name\": \"get_current_weather\",\n",
        "         \"description\": \"Get the current weather in a given location\",\n",
        "         \"parameters\": {\n",
        "             \"type\": \"object\",\n",
        "             \"properties\": {\n",
        "                 \"location\": {\n",
        "                     \"type\": \"string\",\n",
        "                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                 },\n",
        "                 \"unit\": {\n",
        "                     \"type\": \"string\",\n",
        "                     \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "                 }\n",
        "             },\n",
        "             \"required\": [\"location\"]\n",
        "         }\n",
        "     }\n",
        "     ]\n",
        ")\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f399a1d",
      "metadata": {
        "id": "0f399a1d"
      },
      "source": [
        "See the extra `additional_kwargs` that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b70f23",
      "metadata": {
        "id": "c2b70f23"
      },
      "source": [
        "### **Text Embedding Model**\n",
        "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
        "\n",
        "*BTW: Semantic means 'relating to meaning in language or logic.'*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1655de82",
      "metadata": {
        "id": "1655de82"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a2c85e7e",
      "metadata": {
        "id": "a2c85e7e"
      },
      "outputs": [],
      "source": [
        "text = \"Hi! It's time for the beach\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ddc5a368",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddc5a368",
        "outputId": "e25e5279-5856-4ad2-e5a0-fefcade18cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a sample: [0.028720786795020103, -0.00523094879463315, -0.02701309695839882, -0.03908834978938103, 0.051242247223854065]...\n",
            "Your embedding is length 768\n"
          ]
        }
      ],
      "source": [
        "text_embedding = embeddings.embed_query(text)\n",
        "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
        "print (f\"Your embedding is length {len(text_embedding)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38fe99f",
      "metadata": {
        "id": "c38fe99f"
      },
      "source": [
        "## Prompts - Text generally used as instructions to your model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9318ed",
      "metadata": {
        "id": "8b9318ed"
      },
      "source": [
        "### **Prompt**\n",
        "What you'll pass to the underlying model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2d270239",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d270239",
        "outputId": "2335253e-4639-481a-8429-51d1d277d853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The statement is wrong because Tuesday comes after Monday. Wednesday is two days after Monday. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "llm = GoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "# I like to use three double quotation marks for my prompts because it's easier to read\n",
        "prompt = \"\"\"\n",
        "Today is Monday, tomorrow is Wednesday.\n",
        "\n",
        "What is wrong with that statement?\n",
        "\"\"\"\n",
        "\n",
        "print(llm(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74988254",
      "metadata": {
        "id": "74988254"
      },
      "source": [
        "### **Prompt Template**\n",
        "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
        "\n",
        "Think of it as an [f-string](https://realpython.com/python-f-strings/) in python but for prompts\n",
        "\n",
        "*Advanced: Check out LangSmithHub(https://smith.langchain.com/hub) for many more communit prompt templates*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "abcc212d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abcc212d",
        "outputId": "0fe57aab-d6f7-435e-ad9d-b999e686653d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Prompt: \n",
            "I really want to travel to Delhi. What should I do there?\n",
            "\n",
            "Respond in one short sentence\n",
            "\n",
            "-----------\n",
            "LLM Output: Explore historical sites, immerse yourself in the vibrant culture, and savor the delicious street food. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Notice \"location\" below, that is a placeholder for another value later\n",
        "template = \"\"\"\n",
        "I really want to travel to {location}. What should I do there?\n",
        "\n",
        "Respond in one short sentence\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"location\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location='Delhi')\n",
        "\n",
        "print (f\"Final Prompt: {final_prompt}\")\n",
        "print (\"-----------\")\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b43cec2",
      "metadata": {
        "id": "7b43cec2"
      },
      "source": [
        "## Indexes - Structuring documents to LLMs can work with them"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f904e9",
      "metadata": {
        "id": "d3f904e9"
      },
      "source": [
        "### **Document Loaders**\n",
        "Easy ways to import data from other sources. Shared functionality with [OpenAI Plugins](https://openai.com/blog/chatgpt-plugins) [specifically retrieval plugins](https://github.com/openai/chatgpt-retrieval-plugin)\n",
        "\n",
        "See a [big list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. A bunch more on [Llama Index](https://llamahub.ai/) as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4719d4",
      "metadata": {
        "id": "7d4719d4"
      },
      "source": [
        "**HackerNews**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QWIfCZmopez",
        "outputId": "b5cb20b6-9763-42ae-cfd4-5d1ed3004074"
      },
      "id": "1QWIfCZmopez",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "ba88e05b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba88e05b",
        "outputId": "bf5557d7-dfd7-40af-b928-cba3b54a2149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'https://shreysatapara.github.io/index.html', 'title': 'Shrey Satapara', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\nShrey Satapara\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout Me\\n\\n            I am Shrey Satapara, a master's student in the Department\\n              of Artificial Intelligence at Indian Institute of\\n              Technology Hyderabad (IITH), India. Currently, I am advised by Srijith PK and Maunendra Desarkar.\\n          \\n\\n            My research interests lie in Multilingual Natural Language Processing. As a multilingual individual, I am\\n            committed to developing cross-lingual/multilingual models that can support various applications in\\n            low-resource languages. My master's thesis focuses on continual learning and cross-lingual transfer in\\n            multilingual NLP.\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          I started my research journey at LDRP Institute of\\n            Technology and Research, India, where I pursued a Bachelor's in Computer Engineering. My undergraduate\\n          research involved detecting hate speech in\\n            code-mixed dialogues on social media platforms, supervised by Sandip Modha and Thomas Mandl.\\n          Additionally, I worked as a research intern at the Indian\\n            Institute of Management Ahmedabad under the guidance of Hyokjin Kwak, where I\\n          analyzed the impact of Amul Hits by analyzing the user interactions on various social media platforms.\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://shreysatapara.github.io/index.html\")\n",
        "data = loader.load()\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c564583f",
      "metadata": {
        "id": "c564583f"
      },
      "source": [
        "**Books from Gutenberg Project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "72964fb8",
      "metadata": {
        "id": "72964fb8"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import GutenbergLoader\n",
        "\n",
        "loader = GutenbergLoader(\"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\")\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "47140a26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47140a26",
        "outputId": "36c69650-5b50-45c2-e6aa-72e347ba830d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o.—_Seneca_.\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "      At Paris, just after dark one gusty evening in the autumn of 18-,\r\n",
            "\n",
            "\n",
            "      I was enjoying the twofold l\n"
          ]
        }
      ],
      "source": [
        "print(data[0].page_content[1855:1984])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f1386b0",
      "metadata": {
        "id": "7f1386b0"
      },
      "source": [
        "**URLs and webpages**\n",
        "\n",
        "Let's try it out with [Paul Graham's website](http://www.paulgraham.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9601db",
      "metadata": {
        "id": "0e9601db"
      },
      "source": [
        "### **Text Splitters**\n",
        "Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.\n",
        "\n",
        "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) to see which is best for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95713e57",
      "metadata": {
        "id": "95713e57"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a54455f5",
      "metadata": {
        "id": "a54455f5",
        "outputId": "415e00ce-f816-4648-c760-bb77356de279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 1 document\n"
          ]
        }
      ],
      "source": [
        "# This is a long document we can split up.\n",
        "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
        "    pg_work = f.read()\n",
        "\n",
        "print (f\"You have {len([pg_work])} document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19acb18",
      "metadata": {
        "id": "d19acb18"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 150,\n",
        "    chunk_overlap  = 20,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([pg_work])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3090f05",
      "metadata": {
        "id": "e3090f05",
        "outputId": "b387f2b7-58ad-41b3-de03-51523c25e939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 610 documents\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(texts)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a0f45a",
      "metadata": {
        "id": "87a0f45a",
        "outputId": "9dc1d2ab-0bec-4f1e-e7fc-7c6f87971c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview:\n",
            "February 2021Before college the two main things I worked on, outside of school,\n",
            "were writing and programming. I didn't write essays. I wrote what \n",
            "\n",
            "beginning writers were supposed to write then, and probably still\n",
            "are: short stories. My stories were awful. They had hardly any plot,\n"
          ]
        }
      ],
      "source": [
        "print (\"Preview:\")\n",
        "print (texts[0].page_content, \"\\n\")\n",
        "print (texts[1].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9e670d",
      "metadata": {
        "id": "ad9e670d"
      },
      "source": [
        "There are a ton of different ways to do text splitting and it really depends on your retrieval strategy and application design. Check out more splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f85defb",
      "metadata": {
        "id": "1f85defb"
      },
      "source": [
        "### **Retrievers**\n",
        "Easy way to combine documents with language models.\n",
        "\n",
        "There are many different types of retrievers, the most widely supported is the VectoreStoreRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cccbd82",
      "metadata": {
        "id": "8cccbd82"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dab1c20",
      "metadata": {
        "id": "1dab1c20"
      },
      "outputs": [],
      "source": [
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get embedding engine ready\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
        "\n",
        "# Embedd your texts\n",
        "db = FAISS.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62372be",
      "metadata": {
        "id": "e62372be"
      },
      "outputs": [],
      "source": [
        "# Init your retriever. Asking for just 1 document back\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0534bbd",
      "metadata": {
        "id": "e0534bbd",
        "outputId": "0fdc4c8c-99b4-4bc8-9c07-04f4f30ae032"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f8389169070>)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPmSSJSjp3_X"
      },
      "id": "xPmSSJSjp3_X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3846a3b5",
      "metadata": {
        "id": "3846a3b5"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db383cc8",
      "metadata": {
        "id": "db383cc8",
        "outputId": "6b792dff-3528-4cd0-c4b9-4d33c53d30a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "standards; what was the point? No one else wanted one either, so\n",
            "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
            "last.In this di\n",
            "\n",
            "much of it in grad school.Computer Science is an uneasy alliance between two halves, theory\n",
            "and systems. The theory people prove things, and the systems people\n",
            "build things. I wanted to build things. \n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24193139",
      "metadata": {
        "id": "24193139"
      },
      "source": [
        "### **VectorStores**\n",
        "Databases to store vectors. Most popular ones are [Pinecone](https://www.pinecone.io/) & [Weaviate](https://weaviate.io/). More examples on OpenAIs [retriever documentation](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database). [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
        "\n",
        "Conceptually, think of them as tables w/ a column for embeddings (vectors) and a column for metadata.\n",
        "\n",
        "Example\n",
        "\n",
        "| Embedding      | Metadata |\n",
        "| ----------- | ----------- |\n",
        "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
        "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5533ad",
      "metadata": {
        "id": "3c5533ad"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
        "documents = loader.load()\n",
        "\n",
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get embedding engine ready\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661fdf19",
      "metadata": {
        "id": "661fdf19",
        "outputId": "209ad583-b71b-4990-be81-56a6de0e68f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 78 documents\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(texts)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99ac0ea",
      "metadata": {
        "id": "e99ac0ea"
      },
      "outputs": [],
      "source": [
        "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e7758c",
      "metadata": {
        "id": "89e7758c",
        "outputId": "880f1f3d-bdf8-4308-defa-dd2a1e5af0e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 78 embeddings\n",
            "Here's a sample of one: [-0.001058628615053026, -0.01118234211553424, -0.012874804746266883]...\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(embedding_list)} embeddings\")\n",
        "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ac358c5",
      "metadata": {
        "id": "8ac358c5"
      },
      "source": [
        "Your vectorstore store your embeddings (☝️) and make them easily searchable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b9b79b",
      "metadata": {
        "id": "f9b9b79b"
      },
      "source": [
        "## Memory\n",
        "Helping LLMs remember information.\n",
        "\n",
        "Memory is a bit of a loose term. It could be as simple as remembering information you've chatted about in the past or more complicated information retrieval.\n",
        "\n",
        "We'll keep it towards the Chat Message use case. This would be used for chat bots.\n",
        "\n",
        "There are many types of memory, explore [the documentation](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) to see which one fits your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f43b49da",
      "metadata": {
        "id": "f43b49da"
      },
      "source": [
        "### Chat Message History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893a18c1",
      "metadata": {
        "id": "893a18c1"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "history.add_ai_message(\"hi!\")\n",
        "\n",
        "history.add_user_message(\"what is the capital of france?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2949fda",
      "metadata": {
        "id": "a2949fda",
        "outputId": "9f2ef1c9-32af-4bb9-d560-1522510cb8e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='hi!'),\n",
              " HumanMessage(content='what is the capital of france?')]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b74d5cf",
      "metadata": {
        "id": "9b74d5cf",
        "outputId": "53d7c37e-36be-466e-ab9c-4f581200bab7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is Paris.')"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_response = chat(history.messages)\n",
        "ai_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "529e168f",
      "metadata": {
        "id": "529e168f",
        "outputId": "156b3c3f-967a-49a0-cc1e-da669e449597"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='hi!'),\n",
              " HumanMessage(content='what is the capital of france?'),\n",
              " AIMessage(content='The capital of France is Paris.')]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.add_ai_message(ai_response.content)\n",
        "history.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29fc79c",
      "metadata": {
        "id": "f29fc79c"
      },
      "source": [
        "## Chains ⛓️⛓️⛓️\n",
        "Combining different LLM calls and action automatically\n",
        "\n",
        "There are [many applications of chains](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) search to see which are best for your use case.\n",
        "\n",
        "We'll cover one of them:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34ba415",
      "metadata": {
        "id": "c34ba415"
      },
      "source": [
        "### 1. Simple Sequential Chains\n",
        "\n",
        "Easy chains where you can use the output of an LLM as an input into another. Good for breaking up tasks (and keeping your LLM focused)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "79fc0950",
      "metadata": {
        "id": "79fc0950"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "43d4494a",
      "metadata": {
        "id": "43d4494a"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
        "% USER LOCATION\n",
        "{user_location}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
        "\n",
        "# Holds my 'location' chain\n",
        "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b6c8e00f",
      "metadata": {
        "id": "b6c8e00f"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
        "% MEAL\n",
        "{user_meal}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
        "\n",
        "# Holds my 'meal' chain\n",
        "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "7e0b83f2",
      "metadata": {
        "id": "7e0b83f2"
      },
      "outputs": [],
      "source": [
        "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "7d19c64d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d19c64d",
        "outputId": "933dc0fa-c033-475b-fb7c-8837e1293755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mAh, Hyderabad! The name itself conjures up images of majestic architecture and a rich culinary tapestry.  When it comes to classic dishes from Hyderabad, the undisputed king is **Biryani**. \n",
            "\n",
            "More specifically, **Hyderabadi Biryani**. This isn't just any biryani; it's a symphony of flavors and aromas, carefully layered with fragrant basmati rice, tender marinated meat (chicken or mutton are the most traditional), and a heady mix of spices. The \"kachhi\" (raw) style, where the meat and rice are cooked together, is particularly celebrated for its intense flavors. \n",
            "\n",
            "But Hyderabad's culinary delights don't stop there. Other must-try classics include:\n",
            "\n",
            "* **Haleem:** A slow-cooked stew of meat, lentils, and wheat, often enjoyed during the holy month of Ramadan.\n",
            "* **Mirchi ka Salan:** Green chilies simmered in a creamy peanut and sesame gravy, a perfect accompaniment to biryani.\n",
            "* **Qubani ka Meetha:** A royal dessert made with apricots, almonds, and cream, showcasing the city's sweet side.\n",
            "\n",
            "So, if you're ever in Hyderabad, be sure to indulge in these culinary treasures. Your taste buds will thank you! \n",
            "\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m## Simple Chicken Biryani (Hyderabadi Inspired)\n",
            "\n",
            "This recipe simplifies the traditional method for a quicker, easier weeknight meal.\n",
            "\n",
            "**Ingredients:**\n",
            "\n",
            "* 1 lb boneless, skinless chicken thighs, cut into bite-sized pieces\n",
            "* 1 cup basmati rice, rinsed\n",
            "* 1 large onion, sliced\n",
            "* 1-inch ginger, grated\n",
            "* 4 cloves garlic, minced\n",
            "* 2 tbsp biryani masala (store-bought or homemade)\n",
            "* 1/2 cup plain yogurt\n",
            "* 1/4 cup oil\n",
            "* Salt to taste\n",
            "* Fresh cilantro and mint for garnish\n",
            "\n",
            "**Instructions:**\n",
            "\n",
            "1. **Marinate the chicken:** Combine yogurt, 1 tbsp biryani masala, ginger-garlic paste, and salt. Add chicken, mix well, and marinate for at least 30 minutes.\n",
            "2. **Sauté and layer:** Heat oil in a large pot. Sauté onions until golden. Add marinated chicken and cook until browned. Stir in remaining biryani masala and cook for 2 minutes.\n",
            "3. **Add rice and water:** Add rinsed rice to the pot and stir gently. Pour in 2 cups of water, season with salt, and bring to a boil.\n",
            "4. **Simmer and rest:** Reduce heat to low, cover tightly, and simmer for 15-20 minutes, or until rice is cooked and water is absorbed.\n",
            "5. **Garnish and serve:** Fluff rice gently with a fork. Garnish with fresh cilantro and mint. Serve hot with raita or your favorite Indian side dish. \n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "review = overall_chain.run(\"Hyderabad\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yqa2S7SrASa"
      },
      "id": "8yqa2S7SrASa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}